{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit ('jupyterlab': conda)",
      "language": "python",
      "name": "python37564bitjupyterlabcondacf7fca067e6349aa91afc37379c5735b"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "markov_word_gen.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rainermesi/estonian_literature_markov_chain/blob/main/markov_word_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhYgRSz-ew-0"
      },
      "source": [
        "### Generating new words using a Markov Chain and public domain Estonian literature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-phEJn1LheJ"
      },
      "source": [
        "Working with Google Colab, this is needet to mount Google Drive to the notebook for persistent storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpXuoUKUL4hw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef73202e-d705-4553-941c-92610cd72296"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B18XrdtWS33z"
      },
      "source": [
        "Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeVLjk-ALheM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.core.common import flatten\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import unicodedata\n",
        "\n",
        "! pip install BeautifulSoup4\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "\n",
        "! pip install ebooklib\n",
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "import os\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkQwYD0MO4G2"
      },
      "source": [
        "Get list of public domain books. Tartu Public Libary publishes and distributes Estonian classics: https://www.luts.ee/index.php/111-e-raamatud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSJQWrRSO3JY"
      },
      "source": [
        "url = \"https://www.luts.ee/index.php/111-e-raamatud\"\n",
        "req = requests.get(url)\n",
        "soup = BeautifulSoup(req.content, 'html.parser')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcGaJyG_P7i-"
      },
      "source": [
        "list_of_urls = []\n",
        "for i in soup.find_all('a'):\n",
        "  if i.get_text() == 'e-lugerisse (epub)':\n",
        "    list_of_urls.append('https://www.luts.ee'+i.get('href'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDaa-uwMTKcK"
      },
      "source": [
        "For some reason ebooklib does not like getting the epub file straight from requests. So I'm dowloading a copy of the books."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwvOc5-2TCx1"
      },
      "source": [
        "for i in list_of_urls:\n",
        "  r = requests.get(i, allow_redirects=True)\n",
        "  open(i.split('/')[-1], 'wb').write(r.content)\n",
        "  time.sleep(3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM_NnIqXXwdC"
      },
      "source": [
        "Creating a function to parse the epubs into a list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCFQRcFqhP5c"
      },
      "source": [
        "def epub_to_text(corpus):\n",
        "  blacklist = ['[document]','noscript','header','html','meta','head','input','script']\n",
        "  output_str = ''\n",
        "  output_list = []\n",
        "  for i in corpus:\n",
        "    soup = BeautifulSoup(i,'html.parser')\n",
        "    text = soup.find_all(text=True)\n",
        "    for t in text: \n",
        "      if t.parent.name not in blacklist:\n",
        "        output_list.append(unicodedata.normalize(\"NFKD\",t).strip())\n",
        "  output_list = [i.split() for i in output_list]\n",
        "  output_list = list(flatten(output_list))\n",
        "  output_list_copy = [i for i in output_list if len(i) > 2]\n",
        "  output_list = [i.replace(',','') for i in output_list_copy]\n",
        "  output_list = [i.replace(\"'\",'') for i in output_list]\n",
        "  output_list = [i.replace(\"«\",'') for i in output_list]\n",
        "  output_list = [i.replace(\"»\",'') for i in output_list]\n",
        "  output_list = [i.replace(\"(\",'') for i in output_list]\n",
        "  output_list = [i.replace(\")\",'') for i in output_list]\n",
        "  output_list = [i.lower() for i in output_list]\n",
        "  return output_list"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7oautC8X2wl"
      },
      "source": [
        "A final loop to iterate over all the books and join them into one long list of words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVKonlUxX1tS"
      },
      "source": [
        "corpus_word_list = []\n",
        "for filename in os.listdir(os.curdir):\n",
        "  if filename.endswith(\".epub\"):\n",
        "    try:\n",
        "        book = epub.read_epub(filename)\n",
        "        list_of_elements = []\n",
        "        for i in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
        "          list_of_elements.append(i.get_content())\n",
        "        list_of_words = epub_to_text(list_of_elements)\n",
        "        corpus_word_list.extend(list_of_words)\n",
        "    except:\n",
        "      print('Error at element:',filename)\n",
        "  else:\n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9WuFhnwcP-S"
      },
      "source": [
        "Backup the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0q7w79UcSGL"
      },
      "source": [
        "with open('corpus_word_list.txt', 'wb') as fp:\n",
        "    pickle.dump(corpus_word_list, fp)\n",
        "\n",
        "# load the list from disk\n",
        "# with open ('corpus_word_list.txt', 'rb') as fp:\n",
        "#    corpus_word_list = pickle.load(fp)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOGJJhYjetXu"
      },
      "source": [
        "### Analysis of the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-EVgX-0Z5Z7"
      },
      "source": [
        "Prepare the list of words into a Series object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pKp2g1GLhex"
      },
      "source": [
        "pd_word_series = pd.Series(corpus_word_list)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXxLbwKmLheo"
      },
      "source": [
        "What are most popular words in corpus?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGEiSHALhe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aada8d8e-e4b2-4f0c-a7a4-c9b844f0d8ba"
      },
      "source": [
        "pd_word_series.value_counts().head(10)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kui     11852\n",
              "oli      9907\n",
              "aga      9004\n",
              "siis     7645\n",
              "see      6533\n",
              "oma      6134\n",
              "mis      5710\n",
              "nii      4465\n",
              "tema     4199\n",
              "nagu     4179\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmhRYDwoLhfF"
      },
      "source": [
        "How long are words in corpus?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMIU_3JnLhfG"
      },
      "source": [
        "def word_len_df_gen(in_list):\n",
        "    count_list = [len(item) for item in in_list]\n",
        "    count_df = pd.DataFrame.from_dict(Counter(count_list).items())\n",
        "    count_df.sort_values(by=1,ascending=False)\n",
        "    return count_df"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inUlhj8jLhfP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "8b5525ca-62c8-48ec-f1c6-a8e61f551bd5"
      },
      "source": [
        "word_len_df = word_len_df_gen(pd_word_series)\n",
        "word_len_df.sort_values(by=[1],ascending=False).head(10)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>151282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>144065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6</td>\n",
              "      <td>136811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>100647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>97769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>67974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>47942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>10</td>\n",
              "      <td>29018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11</td>\n",
              "      <td>17867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>12</td>\n",
              "      <td>10691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0       1\n",
              "11   4  151282\n",
              "0    5  144065\n",
              "10   6  136811\n",
              "2    7  100647\n",
              "15   3   97769\n",
              "5    8   67974\n",
              "1    9   47942\n",
              "12  10   29018\n",
              "7   11   17867\n",
              "6   12   10691"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSVRMjMubngY"
      },
      "source": [
        "### Creating the Markov Chain to generate new words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7h9YO3LS2U4"
      },
      "source": [
        "Create a dictionay for the graph and parse the wiki corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o_jFRgDSQd2"
      },
      "source": [
        "def create_graph_dict(corpus):\n",
        "  graphdict = defaultdict(lambda:defaultdict(int))\n",
        "  for word in corpus:\n",
        "    prev_letter = word[0]\n",
        "    for letter in word[1:]:\n",
        "      graphdict[prev_letter][letter] += 1\n",
        "      prev_letter = letter\n",
        "  return graphdict"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-GWPnV6SStY"
      },
      "source": [
        "graph_dict = create_graph_dict(pd_word_series)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NqL-Cj0U4FP"
      },
      "source": [
        "Clean up the graph dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6otUu3itUuKY"
      },
      "source": [
        "def graph_cleanup(graph):\n",
        "  abc = ['A', 'a', 'B', 'b', 'D', 'd', 'E', 'e', 'F', 'f', 'G', 'g', 'H', 'h', 'I', 'i', 'J', 'j', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n', 'O', 'o', 'P', 'p', 'R', 'r', 'S', 's', 'Š', 'š', 'Z', 'z', 'Ž', 'ž', 'T', 't', 'U', 'u', 'V', 'v', 'Õ', 'õ', 'Ä', 'ä', 'Ö', 'ö', 'Ü', 'ü']\n",
        "  abc = list(dict.fromkeys(i.lower() for i in abc))\n",
        "  # clean primary keys\n",
        "  tempgraph = dict((k, graph[k]) for k in abc if k in graph) \n",
        "  # clean nested key value pairs\n",
        "  for letter in abc:\n",
        "    try:\n",
        "      for item in tempgraph[letter].copy():\n",
        "        if item not in abc:\n",
        "          del tempgraph[letter][item]\n",
        "    except:\n",
        "        print(letter,'not in corpus as key, skipping letter')\n",
        "  return tempgraph"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSja7rXsV0Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a00ea1-d310-42eb-db76-de794d78c002"
      },
      "source": [
        "graph_dict = graph_cleanup(graph_dict)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "š not in corpus as key, skipping letter\n",
            "ž not in corpus as key, skipping letter\n",
            "õ not in corpus as key, skipping letter\n",
            "ä not in corpus as key, skipping letter\n",
            "ö not in corpus as key, skipping letter\n",
            "ü not in corpus as key, skipping letter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvjoPJmQWL_Y"
      },
      "source": [
        "Create a function to traverse the graph dictionary and create new words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWYADvqQWRnL"
      },
      "source": [
        "def traverse_graph(graph, word_len=3, start_node=None):\n",
        "  \"\"\"Returns a list of words from a randomly weighted walk.\"\"\"\n",
        "  if word_len <= 0:\n",
        "    return []\n",
        "  \n",
        "  # If not given, pick a start node at random.\n",
        "  if not start_node:\n",
        "    start_node = random.choice(list(graph.keys()))\n",
        "  \n",
        "  \n",
        "  weights = np.array(\n",
        "      list(graph[start_node].values()),\n",
        "      dtype=np.float64)\n",
        "  # Normalize letter counts to sum to 1. Create % weights for each letter.\n",
        "  weights /= weights.sum()\n",
        "\n",
        "  # Pick next letter using weighted distribution.\n",
        "  choices = list(graph[start_node].keys())\n",
        "  chosen_letter = np.random.choice(choices, None, p=weights)\n",
        "  \n",
        "  # recursively build a word until word_len = 0\n",
        "  return [chosen_letter] + traverse_graph(\n",
        "      graph, word_len=word_len-1,\n",
        "      start_node=chosen_letter)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTqSTvuKb280"
      },
      "source": [
        "Generate new words with random word lenght."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HjrUzqxWkOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329270ac-190e-4900-b301-dfb74756d324"
      },
      "source": [
        "for i in range(10): \n",
        "  print(''.join(traverse_graph(graph_dict,word_len=random.choice(word_len_df[0]))))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ahkulenoinanguo\n",
            "astlerastaenelikadidik\n",
            "akubedalui\n",
            "nolerakemandevagmala\n",
            "ikatopudadusetasisobuasaelikolisenagealudre\n",
            "emanoolilalmisuagstarituist\n",
            "eeinaigemutedm\n",
            "ilinasoolagiroodaar\n",
            "adeseagagautolinesisimmadahesulgarodamaseel\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRsuNusvb8dZ"
      },
      "source": [
        "Generate new words with word lenght 5 charaters (2nd most popular lenght in corpus)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1maIja60XY0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad82e89-2b74-4245-a121-7c40a7887083"
      },
      "source": [
        "for i in range(10): \n",
        "  print(''.join(traverse_graph(graph_dict,word_len=5)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "udiko\n",
            "sivae\n",
            "udark\n",
            "oimas\n",
            "otase\n",
            "dsost\n",
            "abedu\n",
            "isste\n",
            "astei\n",
            "hadab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uat9zWfLbKTS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}