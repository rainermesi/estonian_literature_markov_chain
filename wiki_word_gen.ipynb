{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit ('jupyterlab': conda)",
      "language": "python",
      "name": "python37564bitjupyterlabcondacf7fca067e6349aa91afc37379c5735b"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "wiki_word_gen.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rainermesi/estonian_literature_markov_chain/blob/main/wiki_word_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhYgRSz-ew-0"
      },
      "source": [
        "### Generating new words using a Markov Chain and public domain Estonian literature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-phEJn1LheJ"
      },
      "source": [
        "Working with Google Colab, this is needet to mount Google Drive to the notebook for persistent storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpXuoUKUL4hw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B18XrdtWS33z"
      },
      "source": [
        "Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeVLjk-ALheM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9857ab71-5619-48a9-c93d-42943a480a5d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.core.common import flatten\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import unicodedata\n",
        "\n",
        "! pip install BeautifulSoup4\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "\n",
        "! pip install ebooklib\n",
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "import os"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Requirement already satisfied: ebooklib in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from ebooklib) (1.15.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from ebooklib) (4.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkQwYD0MO4G2"
      },
      "source": [
        "Get list of public domain books. Tartu Public Libary publishes and distributes Estonian classics: https://www.luts.ee/index.php/111-e-raamatud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSJQWrRSO3JY"
      },
      "source": [
        "url = \"https://www.luts.ee/index.php/111-e-raamatud\"\n",
        "req = requests.get(url)\n",
        "soup = BeautifulSoup(req.content, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcGaJyG_P7i-"
      },
      "source": [
        "list_of_urls = []\n",
        "for i in soup.find_all('a'):\n",
        "  if i.get_text() == 'e-lugerisse (epub)':\n",
        "    list_of_urls.append('https://www.luts.ee'+i.get('href'))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDaa-uwMTKcK"
      },
      "source": [
        "For some reason ebooklib does not like getting the epub file straight from requests. So I'm dowloading a copy of the books."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwvOc5-2TCx1"
      },
      "source": [
        "for i in list_of_urls:\n",
        "  r = requests.get(i, allow_redirects=True)\n",
        "  open(i.split('/')[-1], 'wb').write(r.content)\n",
        "  time.sleep(5)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM_NnIqXXwdC"
      },
      "source": [
        "A function to parse the epubs into text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVKonlUxX1tS"
      },
      "source": [
        "# code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7oautC8X2wl"
      },
      "source": [
        "A final loop to iterate over books and join them into one\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlzV1glyUEH0"
      },
      "source": [
        "for filename in os.listdir(os.curdir):\n",
        "    if filename.endswith(\".epub\"):\n",
        "        print(filename)\n",
        "    else:\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikx68xcQbBMx"
      },
      "source": [
        "Read in Tammsaare corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7agOb2tDbdiJ"
      },
      "source": [
        "book = epub.read_epub(r'/content/drive/My Drive/DATA/Wikipedia/etwiki_latest/Anton_Hansen_Tammsaare_Tode_ja_oigus_I.epub')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKBECrpNbf9p"
      },
      "source": [
        "book_corpus = []\n",
        "\n",
        "for i in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
        "  book_corpus.append(i.get_content())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCFQRcFqhP5c"
      },
      "source": [
        "def epub_to_text(corpus):\n",
        "  blacklist = ['[document]','noscript','header','html','meta','head','input','script']\n",
        "  output_str = ''\n",
        "  output_list = []\n",
        "  for i in corpus:\n",
        "    soup = BeautifulSoup(i,'html.parser')\n",
        "    text = soup.find_all(text=True)\n",
        "    for t in text: \n",
        "      if t.parent.name not in blacklist:\n",
        "        output_list.append(unicodedata.normalize(\"NFKD\",t).strip())\n",
        "  output_list = [i.split() for i in output_list]\n",
        "  output_list = list(flatten(output_list))\n",
        "  output_list_copy = [i for i in output_list if len(i) > 2]\n",
        "  output_list = [i.replace(',','') for i in output_list_copy]\n",
        "  output_list = [i.replace(\"'\",'') for i in output_list]\n",
        "  output_list = [i.replace(\"«\",'') for i in output_list]\n",
        "  output_list = [i.replace(\"»\",'') for i in output_list]\n",
        "  output_list = [i.lower() for i in output_list]\n",
        "  return output_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWHq29MeLOQK"
      },
      "source": [
        "book_word_list = epub_to_text(book_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc2Qh8OYMm0m"
      },
      "source": [
        "book_word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOGJJhYjetXu"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXxLbwKmLheo"
      },
      "source": [
        "What are most popular words in corpus?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pKp2g1GLhex"
      },
      "source": [
        "pd_word_series = pd.Series(book_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGEiSHALhe8"
      },
      "source": [
        "pd_word_series.value_counts().head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmhRYDwoLhfF"
      },
      "source": [
        "How long are words in corpus?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMIU_3JnLhfG"
      },
      "source": [
        "def word_len_df_gen(in_list):\n",
        "    count_list = [len(item) for item in in_list]\n",
        "    count_df = pd.DataFrame.from_dict(Counter(count_list).items())\n",
        "    count_df.sort_values(by=1,ascending=False)\n",
        "    return count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inUlhj8jLhfP"
      },
      "source": [
        "word_len_df = word_len_df_gen(pd_word_series)\n",
        "word_len_df.sort_values(by=[1],ascending=False).head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7h9YO3LS2U4"
      },
      "source": [
        "Create a dictionay for the graph and parse the wiki corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o_jFRgDSQd2"
      },
      "source": [
        "def create_graph_dict(corpus):\n",
        "  graphdict = defaultdict(lambda:defaultdict(int))\n",
        "  for word in corpus:\n",
        "    prev_letter = word[0]\n",
        "    for letter in word[1:]:\n",
        "      graphdict[prev_letter][letter] += 1\n",
        "      prev_letter = letter\n",
        "  return graphdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-GWPnV6SStY"
      },
      "source": [
        "graph_dict = create_graph_dict(pd_word_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NqL-Cj0U4FP"
      },
      "source": [
        "Clean up the graph dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6otUu3itUuKY"
      },
      "source": [
        "def graph_cleanup(graph):\n",
        "  abc = ['A', 'a', 'B', 'b', 'D', 'd', 'E', 'e', 'F', 'f', 'G', 'g', 'H', 'h', 'I', 'i', 'J', 'j', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n', 'O', 'o', 'P', 'p', 'R', 'r', 'S', 's', 'Š', 'š', 'Z', 'z', 'Ž', 'ž', 'T', 't', 'U', 'u', 'V', 'v', 'Õ', 'õ', 'Ä', 'ä', 'Ö', 'ö', 'Ü', 'ü']\n",
        "  abc = list(dict.fromkeys(i.lower() for i in abc))\n",
        "  # clean primary keys\n",
        "  tempgraph = dict((k, graph[k]) for k in abc if k in graph) \n",
        "  # clean nested key value pairs\n",
        "  for letter in abc:\n",
        "    try:\n",
        "      for item in tempgraph[letter].copy():\n",
        "        if item not in abc:\n",
        "          del tempgraph[letter][item]\n",
        "    except:\n",
        "        print(letter,'not in corpus as key, skipping letter')\n",
        "  return tempgraph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSja7rXsV0Bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3e6e7168-1d17-4fc8-be86-77887dc7358c"
      },
      "source": [
        "graph_dict = graph_cleanup(graph_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f  not in corpus as key\n",
            "š  not in corpus as key\n",
            "z  not in corpus as key\n",
            "ž  not in corpus as key\n",
            "õ  not in corpus as key\n",
            "ä  not in corpus as key\n",
            "ö  not in corpus as key\n",
            "ü  not in corpus as key\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvjoPJmQWL_Y"
      },
      "source": [
        "Traverse the graph_dict and create new words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWYADvqQWRnL"
      },
      "source": [
        "def traverse_graph(graph, word_len=3, start_node=None):\n",
        "  \"\"\"Returns a list of words from a randomly weighted walk.\"\"\"\n",
        "  if word_len <= 0:\n",
        "    return []\n",
        "  \n",
        "  # If not given, pick a start node at random.\n",
        "  if not start_node:\n",
        "    start_node = random.choice(list(graph.keys()))\n",
        "  \n",
        "  \n",
        "  weights = np.array(\n",
        "      list(graph[start_node].values()),\n",
        "      dtype=np.float64)\n",
        "  # Normalize letter counts to sum to 1. Create % weights for each letter.\n",
        "  weights /= weights.sum()\n",
        "\n",
        "  # Pick next letter using weighted distribution.\n",
        "  choices = list(graph[start_node].keys())\n",
        "  chosen_letter = np.random.choice(choices, None, p=weights)\n",
        "  \n",
        "  # recursively build a word until word_len = 0\n",
        "  return [chosen_letter] + traverse_graph(\n",
        "      graph, word_len=word_len-1,\n",
        "      start_node=chosen_letter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HjrUzqxWkOT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "02b35211-75fd-495a-9d16-898c81b60bbd"
      },
      "source": [
        "for i in range(10): \n",
        "  print(''.join(traverse_graph(graph_dict,word_len=random.choice(word_len_df[0]))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ugakuisuudauistuhtta\n",
            "aava\n",
            "amama\n",
            "telearit\n",
            "sehimitesessetamimohela\n",
            "amakehatsisistugajui\n",
            "udalkubravagiloo\n",
            "sstaragidindale\n",
            "umelmidrulesaredmeev\n",
            "orukiha\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1maIja60XY0r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}